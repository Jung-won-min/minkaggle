{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11596456,"sourceType":"datasetVersion","datasetId":7272203}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\ntrain = pd.read_csv('/kaggle/input/mydatafile/train.csv')\ntest = pd.read_csv('/kaggle/input/mydatafile/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/mydatafile/sample_submission.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:15:38.559549Z","iopub.execute_input":"2025-04-30T00:15:38.559872Z","iopub.status.idle":"2025-04-30T00:15:38.985827Z","shell.execute_reply.started":"2025-04-30T00:15:38.559835Z","shell.execute_reply":"2025-04-30T00:15:38.984549Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mydatafile/sample_submission.csv\n/kaggle/input/mydatafile/train.csv\n/kaggle/input/mydatafile/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:23:08.866491Z","iopub.execute_input":"2025-04-30T00:23:08.866822Z","iopub.status.idle":"2025-04-30T00:23:08.873472Z","shell.execute_reply.started":"2025-04-30T00:23:08.866799Z","shell.execute_reply":"2025-04-30T00:23:08.872218Z"}},"outputs":[{"name":"stdout","text":"Index(['ID', '설립연도', '국가', '분야', '투자단계', '직원 수', '인수여부', '상장여부', '고객수(백만명)',\n       '총 투자금(억원)', '연매출(억원)', 'SNS 팔로워 수(백만명)', '기업가치(백억원)', '성공확률',\n       'weight'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:15:38.987817Z","iopub.execute_input":"2025-04-30T00:15:38.988154Z","iopub.status.idle":"2025-04-30T00:15:43.988714Z","shell.execute_reply.started":"2025-04-30T00:15:38.988122Z","shell.execute_reply":"2025-04-30T00:15:43.987568Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.1)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install pytorch-tabnet\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\n\nimport torch\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:15:43.990015Z","iopub.execute_input":"2025-04-30T00:15:43.990446Z","iopub.status.idle":"2025-04-30T00:17:16.272210Z","shell.execute_reply.started":"2025-04-30T00:15:43.990413Z","shell.execute_reply":"2025-04-30T00:17:16.271426Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorch-tabnet\n  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.26.4)\nRequirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.2.2)\nRequirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.15.2)\nRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.5.1+cu124)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->pytorch-tabnet) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->pytorch-tabnet) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->pytorch-tabnet) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->pytorch-tabnet) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->pytorch-tabnet) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->pytorch-tabnet) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->pytorch-tabnet) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->pytorch-tabnet) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->pytorch-tabnet) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->pytorch-tabnet) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->pytorch-tabnet) (2024.2.0)\nDownloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n# 예시: 범주형 컬럼, 숫자형 컬럼, 불린형 컬럼 목록\ncat_feats  = ['설립연도','국가','분야','투자단계','기업가치(백억원)']\nnum_feats  = ['직원 수','고객수(백만명)','총 투자금(억원)','연매출(억원)','SNS 팔로워 수(백만명)']\nbool_feats = ['인수여부','상장여부']\n\n# 결측 처리\nfor f in cat_feats + bool_feats:\n    train[f] = train[f].fillna('Missing')\n    test [f] = test [f].fillna('Missing')\n\nfor f in num_feats:\n    train[f] = train[f].fillna(train[f].median())\n    test [f] = test [f].fillna(train[f].median())\n\n# 레이블 인코딩(범주형)\nfor f in cat_feats + bool_feats:\n    le = LabelEncoder()\n    le.fit(pd.concat([train[f], test[f]], axis=0))\n    train[f] = le.transform(train[f])\n    test [f] = le.transform(test[f])\n\n# 목표변수와 가중치 계산\ny = train['성공확률']  # 실제 기업 성공 확률\n# 빈도 기반 가중치: label별 빈도의 역수\nfreq = y.value_counts().to_dict()\ntrain['weight'] = y.map(lambda v: 1.0 / freq[v])\n\n# 학습에 사용할 피쳐 리스트\nfeatures = cat_feats + num_feats + bool_feats\n\n# 3) Weighted MAE 함수 정의\ndef weighted_mae(preds, dataset):\n    labels = dataset.get_label()\n    weights = dataset.get_weight()\n    wm = np.sum(weights * np.abs(labels - preds)) / np.sum(weights)\n    return 'wmae', wm, False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:19:03.469507Z","iopub.execute_input":"2025-04-30T00:19:03.469842Z","iopub.status.idle":"2025-04-30T00:19:03.505578Z","shell.execute_reply.started":"2025-04-30T00:19:03.469819Z","shell.execute_reply":"2025-04-30T00:19:03.503085Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"pip install --upgrade lightgbm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:21:08.038181Z","iopub.execute_input":"2025-04-30T00:21:08.039467Z","iopub.status.idle":"2025-04-30T00:21:12.965711Z","shell.execute_reply.started":"2025-04-30T00:21:08.039414Z","shell.execute_reply":"2025-04-30T00:21:12.964614Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\nCollecting lightgbm\n  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\nRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.0->lightgbm) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.0->lightgbm) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.0->lightgbm) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.0->lightgbm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.0->lightgbm) (2024.2.0)\nDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightgbm\n  Attempting uninstall: lightgbm\n    Found existing installation: lightgbm 4.5.0\n    Uninstalling lightgbm-4.5.0:\n      Successfully uninstalled lightgbm-4.5.0\nSuccessfully installed lightgbm-4.6.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom lightgbm import early_stopping, log_evaluation\n\n# 2) TARGET & 가중치\nTARGET = '성공확률'\ny = train[TARGET].values\nfreq = pd.Series(y).value_counts().to_dict()\nweights = np.array([1.0/freq[v] for v in y])\n\nfeatures = cat_feats + num_feats + bool_feats\n\n# 4) 5-폴드 CV\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\noof_preds = np.zeros(len(train))\ntest_preds = np.zeros(len(test))\n\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(train)):\n    X_tr, X_val = train.iloc[tr_idx][features], train.iloc[val_idx][features]\n    y_tr, y_val = y[tr_idx],               y[val_idx]\n    w_tr, w_val = weights[tr_idx],         weights[val_idx]\n\n    dtrain = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n    dvalid = lgb.Dataset(X_val, label=y_val, weight=w_val)\n\n    params = {\n        'objective': 'regression_l1',\n        'metric': 'None',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'num_leaves': 31,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'seed': 42,\n    }\n\n    model = lgb.train(\n        params,\n        dtrain,\n        num_boost_round=5000,\n        valid_sets=[dvalid],           # 또는 [dtrain, dvalid]\n        feval=weighted_mae,\n        callbacks=[\n            early_stopping(stopping_rounds=100),\n            log_evaluation(period=100)\n        ]\n    )\n\n    oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n    test_preds       += model.predict(test[features], num_iteration=model.best_iteration) / kf.n_splits\n\n# 5) OOF 평가\nwmae_oof = np.sum(weights * np.abs(y - oof_preds)) / np.sum(weights)\nprint(f'OOF Weighted MAE: {wmae_oof:.6f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:27:07.730194Z","iopub.execute_input":"2025-04-30T00:27:07.731631Z","iopub.status.idle":"2025-04-30T00:27:12.604944Z","shell.execute_reply.started":"2025-04-30T00:27:07.731587Z","shell.execute_reply":"2025-04-30T00:27:12.604111Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000416 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1163\n[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 12\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.500000\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's wmae: 0.211966\n[200]\tvalid_0's wmae: 0.210972\n[300]\tvalid_0's wmae: 0.210898\n[400]\tvalid_0's wmae: 0.210559\nEarly stopping, best iteration is:\n[362]\tvalid_0's wmae: 0.210457\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1163\n[LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.500000\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's wmae: 0.212773\n[200]\tvalid_0's wmae: 0.212539\n[300]\tvalid_0's wmae: 0.212333\n[400]\tvalid_0's wmae: 0.212213\nEarly stopping, best iteration is:\n[390]\tvalid_0's wmae: 0.212106\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1162\n[LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.600000\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's wmae: 0.210625\n[200]\tvalid_0's wmae: 0.210071\n[300]\tvalid_0's wmae: 0.209158\n[400]\tvalid_0's wmae: 0.209049\n[500]\tvalid_0's wmae: 0.2092\nEarly stopping, best iteration is:\n[416]\tvalid_0's wmae: 0.208936\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000520 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1162\n[LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.600000\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's wmae: 0.210169\n[200]\tvalid_0's wmae: 0.209804\n[300]\tvalid_0's wmae: 0.2099\nEarly stopping, best iteration is:\n[238]\tvalid_0's wmae: 0.209571\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000480 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1162\n[LightGBM] [Info] Number of data points in the train set: 3501, number of used features: 12\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 0.600000\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's wmae: 0.214144\n[200]\tvalid_0's wmae: 0.212873\n[300]\tvalid_0's wmae: 0.211928\n[400]\tvalid_0's wmae: 0.211341\n[500]\tvalid_0's wmae: 0.21112\n[600]\tvalid_0's wmae: 0.210988\n[700]\tvalid_0's wmae: 0.210899\nEarly stopping, best iteration is:\n[631]\tvalid_0's wmae: 0.210817\nOOF Weighted MAE: 0.210377\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport glob\n\n# 1) Kaggle input 디렉터리에서 sample_submission 파일 검색\ncandidates = glob.glob('/kaggle/input/**/sample_submission*.csv', recursive=True)\nif not candidates:\n    raise FileNotFoundError(\"Kaggle input 디렉터리에서 sample_submission 파일을 찾을 수 없습니다.\")\nsubmission_path = candidates[0]\nprint(\"읽어들일 sample_submission:\", submission_path)\n\n# 2) 읽어서 예측값 덮어쓰기\nsubmission = pd.read_csv(submission_path)\npred_col = submission.columns[1]   # 보통 ID 다음에 오는 예측 칼럼\nsubmission[pred_col] = test_preds\n\n# 3) Kaggle working 디렉터리에 저장\noutput_path = '/kaggle/working/my_best_submission.csv'\nsubmission.to_csv(output_path, index=False)\nprint(f\"저장 완료: {output_path} (pred_col='{pred_col}')\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T00:30:05.867858Z","iopub.execute_input":"2025-04-30T00:30:05.868619Z","iopub.status.idle":"2025-04-30T00:30:05.924095Z","shell.execute_reply.started":"2025-04-30T00:30:05.868593Z","shell.execute_reply":"2025-04-30T00:30:05.923105Z"}},"outputs":[{"name":"stdout","text":"읽어들일 sample_submission: /kaggle/input/mydatafile/sample_submission.csv\n저장 완료: /kaggle/working/my_best_submission.csv (pred_col='성공확률')\n","output_type":"stream"}],"execution_count":21}]}